{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Eandrushenko/CPTS-437/blob/master/HW4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvvf99NRKOxK",
        "colab_type": "text"
      },
      "source": [
        "Elijah Andrushenko\n",
        "03-05-2018\n",
        "CPTS 437\n",
        "Introduction to Machine Learning\n",
        "Homework # 4\n",
        "---\n",
        "1.\n",
        "Error done with calculator by hand:\n",
        "My calculations:\n",
        "•Take Y columns and find the average. Average was 126.7.\n",
        "•Error equals each value in Y minus the Average.\n",
        "•Square each value that was subtracted from the average.\n",
        "•Find the average of the those values.\n",
        "•The Error squared was 4766.7065\n",
        "\n",
        "New Theta0 = 7.42979\n",
        "New Theta1 = -0.00284\n",
        "\n",
        "Thetas were calculated by the first code block below, code for this part was found by a link referenced in that code block.\n",
        "\n",
        "2.\n",
        "https://prnt.sc/mu3l24\n",
        "https://faculty.washington.edu/heagerty/Books/Biostatistics/TABLES/t-Tables/\n",
        "\n",
        "3.\n",
        "**•The bigger the  λ**\n",
        "-The smaller the w\n",
        "-The larger the variance\n",
        "-Not so great with high complexity datasets\n",
        "\n",
        "**•The smaller the λ**\n",
        "-The bigger the w\n",
        "-The larger the bias\n",
        "-Typical to overfit\n",
        "\n",
        "4.\n",
        "•The project title\n",
        "      Binary Planet Classification\n",
        "•The machine learning methods you propose to element\n",
        "      The goal of our project is to be able to classify a planet as either life supporting or non-life supporting. We think that if we can get enough data, with the correct features then we should be able to use a trained K-N-N model to identify new planets that our classifier hasn't seen as either possibility for life or no possibility for life\n",
        "•The dataset(s) you will use to evaluate your methods\n",
        "  We have found two data sets on kaggle that we can use to develop our project, Exoplantes Database and Open Exoplanet Catalogue. Since these data sets are not labeled, We are going to have to find a way of providing labels because K-N-N is a supervised learning model. I think we can get around this by using K-means clustering to provide us with a list of clusters which we can then use as our labels for the K-N-N algorithm.\n",
        "•The performance objective and measures you will use\n",
        "    Since scientists have been able to identify some planets that may actually have a possibility for life we can use these planets as our test values. However one problem is that we will have very few test values so we may have to employ some techniques that we have learned about in class to boost the number of points in a data set.\n",
        "•The names and roles of every person on your project team\n",
        "  Jake Gergen , Elijah Andrushenko\n",
        "•What you hope to learn from the project and how the results could be used by experts\n",
        "  We hope to be able to accuratly predict which planets have a possbility for life and which ones don't, as well as hopfully be able to discover planets that actually do have a possiblity for life that are not yet known.\n",
        " •Milestones:\n",
        " 3/7 - Get project approved, Get a good data set, if not multiple\n",
        " 4/11 - Implementation of machine learning algorithms, verify correct work, graphed results, other visual data represented, begin making other tests, and begin display for professor.\n",
        " Demonstration Data - All work should be complete a day before demonstration date if not sooner.\n",
        " \n",
        "\n",
        "5.\n",
        "Average Accuracies:\n",
        "-Gaussian NB: 85.24\n",
        "-10-fold Binning: 77.05\n",
        "-One VS Rest Accuracy, coded: 72.26\n",
        "-One VS Rest Accuracy, sklearn: 80.22\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNdZFkR2sJiC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#https://towardsdatascience.com/linear-regression-simplified-ordinary-least-square-vs-gradient-descent-48145de2cf76\n",
        "\n",
        "X = [0.07786339, -0.03961813, 0.01103904, -0.04069594, -0.03422907, 0.00564998, 0.08864151, -0.03315126, -0.05686312, -0.03099563, \n",
        "     0.05522933, -0.06009656, 0.00133873, -0.02345095, -0.07410811, 0.01966154, -0.01590626, -0.01590626, 0.03906215, -0.07303030]\n",
        "Y = [233,\t91, 111, 152, 120, 67, 310, 94, 183, 66, 173, 72, 49, 64, 48, 178, 104, 132, 220, 67]\n",
        "\n",
        "t0 = 7.42968\n",
        "t1 = -0.002945\n",
        "a = 0.0001\n",
        "\n",
        "def gradientDescent(X, y, theta, alpha, num_iters):\n",
        "    \"\"\"\n",
        "       Performs gradient descent to learn theta\n",
        "    \"\"\"\n",
        "    m = len(y)  # number of training examples\n",
        "    for i in range(num_iters):\n",
        "        y_hat = np.dot(X, theta)\n",
        "        theta = theta - alpha * (1.0/m) * np.dot(X, y_hat-y)\n",
        "    return theta\n",
        "\n",
        "updated0 = gradientDescent(X, Y, t0, a, 1)\n",
        "updated1 = gradientDescent(X, Y, t1, a, 1)\n",
        "\n",
        "print(updated0)\n",
        "print(updated1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93nVKAPvKO4i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "\n",
        "def read_data():\n",
        "  uploaded = files.upload()\n",
        "  data = np.loadtxt(fname='smarthome.csv', delimiter=',')\n",
        "  X = data[:,:-1]\n",
        "  Y = data[:,-1]\n",
        "  return X,Y\n",
        "\n",
        "X, Y = read_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPwjgU-kbH0M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classlist = np.unique(Y)\n",
        "\n",
        "def build(X,Y):\n",
        "    clf = GaussianNB()\n",
        "    scores = cross_val_score(clf, X, Y, cv = 10)\n",
        "    scores = scores * 100\n",
        "    return scores\n",
        "\n",
        "#Gaussian Naive Bayes\n",
        "accuracy = build(X,Y)\n",
        "print (\"Gaussian NB: \" + str(numpy.mean(accuracy)))  \n",
        "  \n",
        "def discretized(X):\n",
        "  max_features = np.max(X, axis = 0)\n",
        "  row = X.shape[0]\n",
        "  column = X.shape[1]\n",
        "  RA = np.zeros((10,column))\n",
        "  dis = numpy.zeros((row,column), numpy.int8)\n",
        "  for features in range(column): \n",
        "    cur = 0 \n",
        "    step = max_features[features] / 10.00 \n",
        "    RA[9][features] = max_features[features]\n",
        "    for i in range(9):\n",
        "      cur = cur + step\n",
        "      RA[i][features] = cur\n",
        "  for features in range (column):\n",
        "    for i in range(row):\n",
        "      val = X[i][features]\n",
        "      for binning in range(10):\n",
        "        if val > RA[binning][features] :\n",
        "          continue \n",
        "        else:\n",
        "          dis[i][features] = int(binning)\n",
        "          break\n",
        "  return dis \n",
        "\n",
        "#Binning\n",
        "binning = discretized(X)\n",
        "accuracy = build(binning,Y)\n",
        "print(\"10-fold Binning: \" + str(np.mean(accuracy)))\n",
        "\n",
        "def train(X, Y, classes):\n",
        "    instances = X.shape[0]\n",
        "    gaussian = []\n",
        "    for c in classes:\n",
        "        binary = numpy.copy(Y)\n",
        "        for i in range(instances):\n",
        "            label = Y[i]\n",
        "            if label == c:\n",
        "                binary[i] = 1\n",
        "            else:\n",
        "                binary[i] = -1\n",
        "        gauss = GaussianNB()\n",
        "        gauss.fit(X, binary)\n",
        "        gaussian.append(gauss)\n",
        "            \n",
        "    return gaussian\n",
        "  \n",
        "def test(clf, X, Y, classes):\n",
        "    score = [0] * len(classes)\n",
        "    instances = X.shape[0] \n",
        "    predictions = np.zeros((Y.shape[0],1), numpy.int8)\n",
        "    for i in range(instances):\n",
        "        current = X[i]\n",
        "        score = [0] * len(classes)\n",
        "        for c in range(len(classes)):\n",
        "            y = clf[c].predict([current])\n",
        "            score[c] = score[c] + y\n",
        "        predictions[i] = np.argmax(score)\n",
        "    correct = 0\n",
        "    for i in range(instances):\n",
        "        if Y[i] == predictions[i]:\n",
        "            correct += 1\n",
        "    return (float(correct) / float(instances))*100\n",
        "\n",
        "#One vs Rest without Sklearn\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.4, random_state=0)\n",
        "classifiers = train(X_train, Y_train, classlist)\n",
        "accuracy = test(classifiers, X_test, Y_test, classlist)\n",
        "print(\"One VS Rest Accuracy: \"  +  str(accuracy))\n",
        "\n",
        "#One vs Rest with Sklearn\n",
        "classifiers = OneVsRestClassifier(GaussianNB())\n",
        "classifiers.fit(X_train, Y_train)\n",
        "accuracy  = classif.score(X_test, Y_test) * 100     \n",
        "print (\"One VS Rest Accuracy: \"  +  str(accuracy))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wzii8RT3hThl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class K_Means:\n",
        "\n",
        "\tdef __init__(self, k =3, tolerance = 0.0001, max_iterations = 500):\n",
        "\t\tself.k = k\n",
        "\t\tself.tolerance = tolerance\n",
        "\t\tself.max_iterations = max_iterations"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}