The results we recieved were unfortunate because we believe both are models to be overfitting on this problem because they both express basically perfect accuarcy. We thought that if we boosted our dataset significantly we would have enough data to avoid overfitting but this doesnt seem to be the case. Even though overfitting may be occuring we still wanted to know what it was our models were learning. We created two bar graph reports that show how much each feature value is considered when making the classification and what we saw was interesting. We expected both bar graphs to look basically identical but that was not the case, the perceptron favored slightly different features than the random forest. This result although unexpected does make sense and supports our end goal, the perceptron is considering less things overall becuase its learning a higher bias model and the random forest tries to consider many features giving it a low bias. We found both models to be highly influenced by features that would influence how hot the surface of the planet would be which we found as compelling evidence that our models were learning to correctly classify a planet as either in the "habitable zone" or "not in the habitable zone". 
